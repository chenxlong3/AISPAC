{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../../graph_data/com-dblp.ungraph.txt\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pair(line, prob):\n",
    "    pattern = r'\\d+'\n",
    "    matches = re.findall(pattern, line)\n",
    "    st, end = matches[0], matches[1]\n",
    "    return f\"{st} {end} {str(prob)}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"./data/DBLP.txt\"\n",
    "cleaned_list = [modify_pair(line, 0.1) for line in data_list[4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\d+'\n",
    "matches = re.findall(pattern, data_list[2])\n",
    "DBLP_n, DBLP_m = matches[0], matches[1]\n",
    "num_nodes = int(DBLP_n)\n",
    "res_list = [DBLP_n + '\\n', DBLP_m + '\\n'] + cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read in edge list file\n",
    "edges = []\n",
    "node_set = set()\n",
    "out_nodes = set()\n",
    "occupied_idx = [False for i in range(num_nodes)]\n",
    "for line in cleaned_list:\n",
    "    match = re.findall(r'\\d+\\.?\\d*', line)\n",
    "    u = int(match[0])\n",
    "    v = int(match[1])\n",
    "    w = float(match[2])\n",
    "    node_set.add(u)\n",
    "    node_set.add(v)\n",
    "    edges.append((u, v, w))\n",
    "\n",
    "    if u >= num_nodes:\n",
    "        out_nodes.add(u)\n",
    "    else:\n",
    "        occupied_idx[u] = True\n",
    "    if v >= num_nodes:\n",
    "        out_nodes.add(v)\n",
    "    else:\n",
    "        occupied_idx[v] = True\n",
    "\n",
    "# Determine the maximum node index\n",
    "max_node_index = max(max(edge[:2]) for edge in edges)\n",
    "\n",
    "# Create a mapping from old node indices to new ones\n",
    "mapping = {}\n",
    "new_index = 0\n",
    "avai_idx = [i for i in range(num_nodes) if occupied_idx[i] == False]\n",
    "for i in range(num_nodes):\n",
    "    if occupied_idx[i]:\n",
    "        mapping[i] = i\n",
    "\n",
    "assert len(mapping.keys()) == num_nodes - len(out_nodes)\n",
    "assert len(avai_idx) == len(out_nodes)\n",
    "for i, node in enumerate(out_nodes):\n",
    "    mapping[node] = avai_idx[i]\n",
    "\n",
    "# Re-index the edges\n",
    "new_edges = [(mapping[u], mapping[v], w) for u, v, w in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the re-indexed edges to a new file\n",
    "new_filename = \"./data/DBLP_1.txt\"\n",
    "\n",
    "with open(new_filename, \"w\") as f:\n",
    "    f.write(DBLP_n + '\\n')\n",
    "    f.write(DBLP_m + '\\n')\n",
    "    for u, v, w in new_edges:\n",
    "        f.write(f\"{u}\\t{v}\\t{w}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_legal(line):\n",
    "    pattern = r'^\\d.*\\d.*\\n$'\n",
    "    return re.match(pattern, line)\n",
    "\n",
    "def get_edge_from_str(edge_str):\n",
    "    pattern = r'\\d+'\n",
    "    matches = re.findall(pattern, edge_str)\n",
    "    return (int(matches[0]), int(matches[1]))\n",
    "\n",
    "def get_triple_from_str(edge_str):\n",
    "    pattern = r\"\\d+\\.\\d+|\\d+\"\n",
    "    matches = re.findall(pattern, edge_str)\n",
    "    return (int(matches[0]), int(matches[1]), float(matches[2]))\n",
    "\n",
    "def read_file(file_name = \"./data/Epinions/soc-Epinions1.txt\"):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data_list = f.readlines()\n",
    "    cleaned_data_list = []\n",
    "    for line in data_list:\n",
    "        if check_line_legal(line):\n",
    "            cleaned_data_list.append(get_edge_from_str(line))\n",
    "    del data_list\n",
    "    return cleaned_data_list\n",
    "\n",
    "def read_cleaned_prob_file(file_name = \"./data/DBLP/edgelist_ic.txt\"):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data_list = f.readlines()\n",
    "    cleaned_data_list = []\n",
    "    for line in data_list:\n",
    "        cleaned_data_list.append(get_triple_from_str(line))\n",
    "    return cleaned_data_list\n",
    "\n",
    "def re_index(edge_list):\n",
    "    node_set = set()\n",
    "    max_node = 0\n",
    "\n",
    "    for edge in edge_list:\n",
    "        node_set.add(edge[0])\n",
    "        node_set.add(edge[1])\n",
    "        max_node = max(max(edge[0], edge[1]), max_node)\n",
    "    \n",
    "    num_nodes = len(node_set)\n",
    "    print(\"The largest index is:\", max_node)\n",
    "    print(\"The total number of nodes is:\", num_nodes)\n",
    "    if max_node <= len(node_set) - 1:\n",
    "        print(\"NOT NEED REINDEXING\")\n",
    "        return None\n",
    "    occupied_idx = [False for i in range(len(node_set))]\n",
    "    out_nodes = []\n",
    "    for node in node_set:\n",
    "        if node >= num_nodes:\n",
    "            out_nodes.append(node)\n",
    "        else:\n",
    "            occupied_idx[node] = True\n",
    "    \n",
    "    avai_idx = [i for i in range(num_nodes) if occupied_idx[i] == False]\n",
    "    mapping = {}\n",
    "    assert(len(avai_idx) == len(out_nodes))\n",
    "    for i, node in enumerate(out_nodes):\n",
    "        mapping[node] = avai_idx[i]\n",
    "    new_edges = []\n",
    "    for (u, v) in edge_list:\n",
    "        new_u = u if u < num_nodes else mapping[u]\n",
    "        new_v = v if v < num_nodes else mapping[v]\n",
    "        new_edges.append((new_u, new_v))\n",
    "    return new_edges\n",
    "\n",
    "def data_preprocessing(filepath=\"./data/Epinions/soc-Epinions1.txt\", p_setting=0.1):\n",
    "    cur_dir = os.path.dirname(filepath)\n",
    "    edge_list = read_file(filepath)\n",
    "    new_edges = re_index(edge_list)\n",
    "    if new_edges == None:\n",
    "        print(\"Dataset is good.\")\n",
    "    with open(os.path.join(cur_dir, 'edgelist_ic.txt'), 'w') as f:\n",
    "        for (u, v) in new_edges:\n",
    "            f.write(f\"{u}\\t{v}\\t{p_setting}\\n\")\n",
    "    return\n",
    "\n",
    "def modify_prob(filepath=\"./data/Epinions/soc-Epinions1.txt\", p_setting=0.1):\n",
    "    cur_dir = os.path.dirname(filepath)\n",
    "    edge_list = read_cleaned_prob_file(filepath)\n",
    "    with open(os.path.join(cur_dir, 'edgelist_ic.txt'), 'w') as f:\n",
    "        for (u, v, w) in edge_list:\n",
    "            f.write(f\"{u}\\t{v}\\t{p_setting}\\n\")\n",
    "    print(\"Finish modifying the probability to\", p_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest index is: 3072440\n",
      "The total number of nodes is: 3072441\n",
      "NOT NEED REINDEXING\n",
      "Dataset is good.\n"
     ]
    }
   ],
   "source": [
    "data_preprocessing(\"./data/orkut/edgelist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/orkut/edgelist_ic.txt\", 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t1\\t0.1\\n',\n",
       " '0\\t2\\t0.1\\n',\n",
       " '0\\t3\\t0.1\\n',\n",
       " '0\\t4\\t0.1\\n',\n",
       " '0\\t5\\t0.1\\n',\n",
       " '0\\t6\\t0.1\\n',\n",
       " '0\\t7\\t0.1\\n',\n",
       " '0\\t8\\t0.1\\n',\n",
       " '0\\t9\\t0.1\\n',\n",
       " '0\\t10\\t0.1\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inf_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return float(df[\"expected spread\"].iloc[-1])\n",
    "MC_res = []\n",
    "for i, rand_seed in enumerate([2020, 2021, 2022, 2023, 2024]):\n",
    "    folder_path = f\"./data/GRQC/params/50_10000_0.500000_0.001000_{rand_seed}_RANDseed_WC/\"\n",
    "    csv_filenames = [os.path.join(folder_path, f\"k_inf_spread_MCGreedy_{i}.csv\") for i in range(1000, 10001, 1000)]\n",
    "    MC_res.append([get_inf_from_csv(filepath) for filepath in csv_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 446.26219999999995)\n",
      "(2, 470.24080000000004)\n",
      "(3, 502.73699999999997)\n",
      "(4, 502.4652)\n",
      "(5, 505.77220000000005)\n",
      "(6, 520.9777999999999)\n",
      "(7, 519.826)\n",
      "(8, 524.028)\n",
      "(9, 522.3344)\n",
      "(10, 522.3902)\n"
     ]
    }
   ],
   "source": [
    "res = np.array(MC_res).mean(axis=0)\n",
    "for i in range(10):\n",
    "    print(f\"({i+1}, {res[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(f\"./data/nethept/params/50_10000_0.500000_0.001000_2023_RANDseed_WC/\", \"k_inf_spread_MCGreedy_1000.csv\"))[\"expected spread\"]\n",
    "test.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_NAMES = [\"RAND\", \"OUTDEG\", \"PROB\", \"SINF\", \"UINF\", \"AIS-U\" , \"IMA\"]\n",
    "RAND_SEEDS = [2021, 2022, 2023, 2024, 2025]\n",
    "K_LIST = [5, 10, 20, 30, 40, 50]\n",
    "EPS_LIST = [\"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "BETA_LIST = [1, 2, 4, 8, 16, 32, 64]\n",
    "rand_seeds_nethept = [42, 2020, 2022, 2023, 2024]\n",
    "\n",
    "def get_res_from_file(file_path, mode=\"result\")->float:\n",
    "    with open(file_path, 'r') as file:\n",
    "        match = None\n",
    "        for line in file:\n",
    "            if mode == \"result\" and line.startswith('Result by'):\n",
    "                match = re.search(r'\\d+(\\.\\d+)?', line)\n",
    "            elif mode == \"origin\" and line.startswith(\"Original Inf:\"):\n",
    "                match = re.search(r'\\d+(\\.\\d+)?', line)\n",
    "            \n",
    "        if match:\n",
    "            number = float(match.group())\n",
    "            return number\n",
    "        else:\n",
    "            print(\"RESULT NOT FOUND\", file_path)\n",
    "\n",
    "def get_RRset_generation_time(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        contents = file.read()\n",
    "\n",
    "        # Use regular expressions to extract the RR set generation time\n",
    "        pattern = r\"->Time used \\(sec\\) for operation \\[RR set generation\\]: (\\d+\\.\\d+|\\d+)\"\n",
    "        match = re.search(pattern, contents)\n",
    "\n",
    "        if match:\n",
    "            rr_set_time = float(match.group(1))\n",
    "            return rr_set_time\n",
    "        else:\n",
    "            print(\"RR set generation time not found\")\n",
    "            return -1.0\n",
    "\n",
    "def get_process_time(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Read the contents of the file\n",
    "        contents = file.read()\n",
    "\n",
    "        # Use regular expressions to find the line with the keyword \"process\"\n",
    "        pattern = r\".*process.*\"\n",
    "        match = re.search(pattern, contents, re.MULTILINE)\n",
    "\n",
    "        if match:\n",
    "            line = match.group(0)\n",
    "            # Use regular expressions to extract the running time from the line\n",
    "            time_pattern = r\"\\d+(\\.\\d+)?\"\n",
    "            time_match = re.search(time_pattern, line)\n",
    "            if time_match:\n",
    "                running_time = float(time_match.group(0))\n",
    "                return running_time\n",
    "            else:\n",
    "                print(\"Running time not found in the line with the keyword 'process'\")\n",
    "        else:\n",
    "            print(\"Line with the keyword 'process' not found\")\n",
    "\n",
    "def get_original_inf(dataset_name:str, rand_seeds):\n",
    "    res_arr = []\n",
    "    for rand_seed in rand_seeds:\n",
    "        path = f\"./data/{dataset_name}/params/5_0_0.500000_0.001000_{rand_seed}_WC/log_IMA.txt\"\n",
    "        res_arr.append(get_res_from_file(path, \"origin\"))\n",
    "    return sum(res_arr) / len(res_arr)\n",
    "\n",
    "def get_inf_beta_of_one_dataset(dataset_name:str, rand_seeds, baseline_names=[\"IMA\"], k_edges=50):\n",
    "    res = {}\n",
    "    for baseline in baseline_names:\n",
    "        inf_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            inf_for_one_rand_seed = []\n",
    "            for beta in BETA_LIST:\n",
    "                try:\n",
    "                    if beta == 1:\n",
    "                        path = f\"./data/{dataset_name}/params/{k_edges}_0_0.500000_0.001000_{rand_seed}_WC/log_{baseline}.txt\"\n",
    "                    else:\n",
    "                        path = f\"./data/{dataset_name}/params/{k_edges}_0_0.500000_0.001000_{rand_seed}_WC_beta_{beta}/log_{baseline}.txt\"\n",
    "                    # print(path)\n",
    "                    inf_for_one_rand_seed.append(get_res_from_file(path, \"result\"))\n",
    "                except:\n",
    "                    print(path)\n",
    "            inf_diff_rand_seed_list.append(inf_for_one_rand_seed)\n",
    "        \n",
    "        x_mean = np.mean(np.array(inf_diff_rand_seed_list), axis=0)\n",
    "        res[baseline] = x_mean / np.max(x_mean)\n",
    "        # res[baseline] = np.mean(np.array(inf_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_time_beta_of_one_dataset(dataset_name:str, rand_seeds, baseline_names=[\"IMA\"], k_edges=50):\n",
    "    res = {}\n",
    "    for baseline in baseline_names:\n",
    "        time_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            time_for_one_rand_seed = []\n",
    "            for beta in BETA_LIST:\n",
    "                try:\n",
    "                    if beta == 1:\n",
    "                        working_folder = f\"./data/{dataset_name}/params/{k_edges}_0_0.500000_0.001000_{rand_seed}_WC/\"\n",
    "                    else:\n",
    "                        working_folder = f\"./data/{dataset_name}/params/{k_edges}_0_0.500000_0.001000_{rand_seed}_WC_beta_{beta}/\"\n",
    "                    \n",
    "                    log_path = os.path.join(working_folder, \"logs.txt\")\n",
    "                    log_baseline_path = os.path.join(working_folder, f\"log_{baseline}.txt\")\n",
    "                    RR_set_gen_time = get_RRset_generation_time(log_path)\n",
    "                    process_time = get_process_time(log_baseline_path)\n",
    "                    time_for_one_rand_seed.append(RR_set_gen_time + process_time)\n",
    "                except Exception as e:\n",
    "                    print(e.args)\n",
    "                    print(baseline, rand_seed, k_edges)\n",
    "                    print(working_folder)\n",
    "            time_diff_rand_seed_list.append(time_for_one_rand_seed)\n",
    "        \n",
    "        res[baseline] = np.mean(np.array(time_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_inf_spread_of_one_dataset(dataset_name:str, rand_seeds):\n",
    "    res = {}\n",
    "    origin_inf = get_original_inf(dataset_name, rand_seeds)\n",
    "    for baseline in BASELINE_NAMES:\n",
    "        inf_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            inf_for_one_rand_seed = [origin_inf]\n",
    "            for k_edges in K_LIST:\n",
    "                try:\n",
    "                    path = f\"./data/{dataset_name}/params/{k_edges}_0_0.500000_0.001000_{rand_seed}_WC/log_{baseline}.txt\"\n",
    "                    # print(path)\n",
    "                    inf_for_one_rand_seed.append(get_res_from_file(path, \"result\"))\n",
    "                except:\n",
    "                    print(baseline, rand_seed, k_edges)\n",
    "                    print(path)\n",
    "            inf_diff_rand_seed_list.append(inf_for_one_rand_seed)\n",
    "        res[baseline] = np.mean(np.array(inf_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_eps_inf_of_one_dataset(dataset_name:str, rand_seeds, baseline_names=[\"IMA\"], k_edges=50):\n",
    "    res = {}\n",
    "    \n",
    "    for baseline in baseline_names:\n",
    "        inf_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            inf_for_one_rand_seed = []\n",
    "            for eps in EPS_LIST:\n",
    "                try:\n",
    "                    path = f\"./data/{dataset_name}/params/{k_edges}_0_{eps}00000_0.001000_{rand_seed}_WC/log_{baseline}.txt\"\n",
    "                    # print(path)\n",
    "                    inf_for_one_rand_seed.append(get_res_from_file(path, \"result\"))\n",
    "                except:\n",
    "                    print(path)\n",
    "            inf_diff_rand_seed_list.append(inf_for_one_rand_seed)\n",
    "        x_mean = np.mean(np.array(inf_diff_rand_seed_list), axis=0)\n",
    "        res[baseline] = x_mean / np.max(x_mean)\n",
    "    return res\n",
    "\n",
    "def get_eps_time_of_one_dataset(dataset_name:str, rand_seeds, baseline_names=[\"IMA\"]):\n",
    "    res = {}\n",
    "    for baseline in baseline_names:\n",
    "        time_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            time_for_one_rand_seed = []\n",
    "            for eps in EPS_LIST:\n",
    "                try:\n",
    "                    working_folder = f\"./data/{dataset_name}/params/50_0_{eps}00000_0.001000_{rand_seed}_WC/\"\n",
    "                    log_path = os.path.join(working_folder, \"logs.txt\")\n",
    "                    log_baseline_path = os.path.join(working_folder, f\"log_{baseline}.txt\")\n",
    "                    RR_set_gen_time = get_RRset_generation_time(log_path)\n",
    "                    process_time = get_process_time(log_baseline_path)\n",
    "                    time_for_one_rand_seed.append(RR_set_gen_time + process_time)\n",
    "                except Exception as e:\n",
    "                    print(e.args)\n",
    "                    print(working_folder)\n",
    "            time_diff_rand_seed_list.append(time_for_one_rand_seed)\n",
    "        res[baseline] = np.mean(np.array(time_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_runtime_of_one_dataset(dataset_name:str, rand_seeds, baseline_names=[\"SINF\", \"UINF\", \"AIS-U\" , \"IMA\"], k_list=K_LIST, seed_mode=\"IM\",\n",
    "                               num_cand_edges=0):\n",
    "    res = {}\n",
    "    for baseline in baseline_names:\n",
    "        time_diff_rand_seed_list = []\n",
    "        for rand_seed in rand_seeds:\n",
    "            time_for_one_rand_seed = []\n",
    "            for k_edges in k_list:\n",
    "                try:\n",
    "                    working_folder = f\"./data/{dataset_name}/params/{k_edges}_{num_cand_edges}_0.500000_0.001000_{rand_seed}_WC/\"\n",
    "                    if seed_mode == \"RAND\":\n",
    "                        working_folder = f\"./data/{dataset_name}/params/{k_edges}_{num_cand_edges}_0.500000_0.001000_{rand_seed}_RANDseed_WC/\"\n",
    "                    log_path = os.path.join(working_folder, \"logs.txt\")\n",
    "                    log_baseline_path = os.path.join(working_folder, f\"log_{baseline}.txt\")\n",
    "                    RR_set_gen_time = get_RRset_generation_time(log_path)\n",
    "                    process_time = get_process_time(log_baseline_path)\n",
    "                    if baseline in [\"SINF\", \"UINF\", \"AIS-U\" , \"IMA\"]:\n",
    "                        time_for_one_rand_seed.append(RR_set_gen_time + process_time)\n",
    "                    else:\n",
    "                        time_for_one_rand_seed.append(process_time)\n",
    "                except Exception as e:\n",
    "                    print(e.args)\n",
    "                    print(working_folder)\n",
    "            time_diff_rand_seed_list.append(time_for_one_rand_seed)\n",
    "        res[baseline] = np.mean(np.array(time_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_runtime_k_mcg(dataset_name:str, rand_seeds:list, k_list=K_LIST, seed_mode=\"RAND\", num_cand_edges=10000):\n",
    "    res = {}\n",
    "\n",
    "    time_diff_rand_seed_list = []\n",
    "    for rand_seed in rand_seeds:\n",
    "        time_for_one_rand_seed = []\n",
    "        try:\n",
    "            working_folder = f\"./data/{dataset_name}/params/50_{num_cand_edges}_0.500000_0.001000_{rand_seed}_WC/\"\n",
    "            if seed_mode == \"RAND\":\n",
    "                working_folder = f\"./data/{dataset_name}/params/50_{num_cand_edges}_0.500000_0.001000_{rand_seed}_RANDseed_WC/\"\n",
    "            log_path = os.path.join(working_folder, \"MCGreedy_10000_time.txt\")\n",
    "            with open(log_path, 'r') as f:\n",
    "                contents = f.read()\n",
    "                for k_edges in k_list:\n",
    "                    pattern = r\"\\[select_\" + str(k_edges) + r\"\\]:\\s+(\\d+(\\.\\d+)?)\"\n",
    "                    match = re.search(pattern, contents)\n",
    "                    time_for_one_rand_seed.append(float(match.group(1)))\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "            print(working_folder)\n",
    "        time_diff_rand_seed_list.append(time_for_one_rand_seed)\n",
    "        res[\"MCGreedy\"] = np.mean(np.array(time_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def get_runtime_r_mcg(dataset_name:str, rand_seeds:list, r_list=[2000, 4000, 6000, 8000, 10000], seed_mode=\"RAND\", num_cand_edges=10000):\n",
    "    res = {}\n",
    "\n",
    "    time_diff_rand_seed_list = []\n",
    "    for rand_seed in rand_seeds:\n",
    "        time_for_one_rand_seed = []\n",
    "        try:\n",
    "            working_folder = f\"./data/{dataset_name}/params/50_{num_cand_edges}_0.500000_0.001000_{rand_seed}_WC/\"\n",
    "            if seed_mode == \"RAND\":\n",
    "                working_folder = f\"./data/{dataset_name}/params/50_{num_cand_edges}_0.500000_0.001000_{rand_seed}_RANDseed_WC/\"\n",
    "            for r in r_list:\n",
    "                log_path = os.path.join(working_folder, f\"MCGreedy_{str(r)}_time.txt\")\n",
    "                with open(log_path, 'r') as f:\n",
    "                    contents = f.read()\n",
    "                    pattern = r\"\\[select_50\\]:\\s+(\\d+(\\.\\d+)?)\"\n",
    "                    match = re.search(pattern, contents)\n",
    "                    time_for_one_rand_seed.append(float(match.group(1)))\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "            print(working_folder)\n",
    "            print(r)\n",
    "        time_diff_rand_seed_list.append(time_for_one_rand_seed)\n",
    "        res[\"MCGreedy\"] = np.mean(np.array(time_diff_rand_seed_list), axis=0)\n",
    "    return res\n",
    "\n",
    "def display_for_tex(method_inf_dict, method=\"IMA\", k_list = [0, 5, 10, 20, 30, 40, 50]):\n",
    "    for i in range(len(method_inf_dict[method])):\n",
    "        print(f\"({k_list[i]},{round(method_inf_dict[method][i], 4)})\")\n",
    "\n",
    "def display_for_tex_all(method_inf_dict, k_list = [0, 5, 10, 20, 30, 40, 50]):\n",
    "    for method in method_inf_dict.keys():\n",
    "        print(method + \":\")\n",
    "        display_for_tex(method_inf_dict, method, k_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Datasets with 10000 candidate edges\n",
    "### Baseline Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime_grqc = get_runtime_of_one_dataset(\"GRQC\", RAND_SEEDS, baseline_names=[\"IMA\"], seed_mode=\"RAND\", num_cand_edges=10000)\n",
    "# runtime_nethept = get_runtime_of_one_dataset(\"nethept\", rand_seeds_nethept, baseline_names=[\"IMA\"], seed_mode=\"RAND\", num_cand_edges=10000)\n",
    "runtime_grqc = get_runtime_of_one_dataset(\"GRQC\", RAND_SEEDS, baseline_names=[\"IMA\"], seed_mode=\"IM\", num_cand_edges=10000)\n",
    "runtime_nethept = get_runtime_of_one_dataset(\"nethept\", rand_seeds_nethept, baseline_names=[\"IMA\"], seed_mode=\"IM\", num_cand_edges=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCGreedy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCG_time_k_grqc = get_runtime_k_mcg(\"GRQC\", RAND_SEEDS)\n",
    "MCG_time_k_nethept = get_runtime_k_mcg(\"nethept\", RAND_SEEDS)\n",
    "runtime_r_GRQC_step2k = get_runtime_r_mcg(\"GRQC\", RAND_SEEDS)\n",
    "runtime_r_nethept_step2k = get_runtime_r_mcg(\"nethept\", RAND_SEEDS)\n",
    "runtime_r_GRQC = get_runtime_r_mcg(\"GRQC\", RAND_SEEDS, [i*1000 for i in range(1, 6)])\n",
    "runtime_r_nethept = get_runtime_r_mcg(\"nethept\", RAND_SEEDS, [i*1000 for i in range(1, 6)])\n",
    "runtime_r_GRQC_IM = get_runtime_r_mcg(\"GRQC\", RAND_SEEDS, [1000, 2000, 3000, 4000, 5000], \"IM\")\n",
    "runtime_r_nethept_IM = get_runtime_r_mcg(\"nethept\", RAND_SEEDS, [1000, 2000, 3000, 4000, 5000], \"IM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,304.4098)\n",
      "(2,326.0024)\n",
      "(3,404.653)\n",
      "(4,434.4754)\n",
      "(5,640.8142)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex(runtime_r_nethept, \"MCGreedy\", [i for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,1510.126)\n",
      "(2000,3024.026)\n",
      "(3000,4641.218)\n",
      "(4000,6418.404)\n",
      "(5000,7780.71)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex(runtime_r_nethept_IM, \"MCGreedy\", [i*1000 for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,0.0957)\n",
      "(10,0.3037)\n",
      "(20,1.0251)\n",
      "(30,2.2648)\n",
      "(40,4.0737)\n",
      "(50,6.259)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex(runtime_nethept, \"IMA\", K_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_beta_nethept = get_inf_beta_of_one_dataset(\"nethept\", rand_seeds_nethept)\n",
    "inf_beta_epinion = get_inf_beta_of_one_dataset(\"Epinions\", RAND_SEEDS)\n",
    "inf_beta_dblp = get_inf_beta_of_one_dataset(\"DBLP\", RAND_SEEDS)\n",
    "inf_beta_orkut = get_inf_beta_of_one_dataset(\"orkut\", RAND_SEEDS)\n",
    "inf_beta_twitter = get_inf_beta_of_one_dataset(\"twitter\", RAND_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMA:\n",
      "(0,0.9984)\n",
      "(1,0.9984)\n",
      "(2,0.9984)\n",
      "(3,0.9982)\n",
      "(4,0.9999)\n",
      "(5,0.9993)\n",
      "(6,1.0)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(inf_beta_twitter, [i for i in range(len(BETA_LIST))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_beta_nethept = get_time_beta_of_one_dataset(\"nethept\", rand_seeds_nethept)\n",
    "time_beta_epinion = get_time_beta_of_one_dataset(\"Epinions\", RAND_SEEDS)\n",
    "time_beta_dblp = get_time_beta_of_one_dataset(\"DBLP\", RAND_SEEDS)\n",
    "time_beta_orkut = get_time_beta_of_one_dataset(\"orkut\", RAND_SEEDS)\n",
    "time_beta_twitter = get_time_beta_of_one_dataset(\"twitter\", RAND_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMA:\n",
      "(0,699.4426)\n",
      "(1,459.037)\n",
      "(2,339.8896)\n",
      "(3,284.2249)\n",
      "(4,243.919)\n",
      "(5,224.9127)\n",
      "(6,222.3034)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(time_beta_twitter, [i for i in range(len(BETA_LIST))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_inf_nethept = get_eps_inf_of_one_dataset(\"nethept\", rand_seeds_nethept)\n",
    "eps_inf_epin = get_eps_inf_of_one_dataset(\"Epinions\", RAND_SEEDS)\n",
    "eps_inf_dblp = get_eps_inf_of_one_dataset(\"DBLP\", RAND_SEEDS)\n",
    "eps_inf_orkut = get_eps_inf_of_one_dataset(\"orkut\", RAND_SEEDS)\n",
    "eps_inf_twitter = get_eps_inf_of_one_dataset(\"twitter\", RAND_SEEDS[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMA:\n",
      "(0.1,1.0)\n",
      "(0.2,1.0)\n",
      "(0.3,0.9999)\n",
      "(0.4,0.9999)\n",
      "(0.5,0.9998)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(eps_inf_twitter, EPS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_time_nethept = get_eps_time_of_one_dataset(\"nethept\", rand_seeds_nethept)\n",
    "eps_time_epin = get_eps_time_of_one_dataset(\"Epinions\", RAND_SEEDS)\n",
    "eps_time_dblp = get_eps_time_of_one_dataset(\"DBLP\", RAND_SEEDS)\n",
    "eps_time_orkut = get_eps_time_of_one_dataset(\"orkut\", RAND_SEEDS)\n",
    "eps_time_twitter = get_eps_time_of_one_dataset(\"twitter\", RAND_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMA:\n",
      "(0.1,12171.155)\n",
      "(0.2,3112.0224)\n",
      "(0.3,1507.3786)\n",
      "(0.4,941.1138)\n",
      "(0.5,699.4426)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(eps_time_twitter, [0.1, 0.2, 0.3, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMA:\n",
      "(0.1,0.9997)\n",
      "(0.2,0.9998)\n",
      "(0.3,1.0)\n",
      "(0.4,0.9999)\n",
      "(0.5,0.9992)\n",
      "IMA:\n",
      "(0.1,1.0)\n",
      "(0.2,0.9999)\n",
      "(0.3,0.9998)\n",
      "(0.4,0.9998)\n",
      "(0.5,1.0)\n",
      "IMA:\n",
      "(0.1,0.9999)\n",
      "(0.2,1.0)\n",
      "(0.3,0.9999)\n",
      "(0.4,1.0)\n",
      "(0.5,0.9997)\n",
      "IMA:\n",
      "(0.1,0.9999)\n",
      "(0.2,0.9999)\n",
      "(0.3,0.9999)\n",
      "(0.4,1.0)\n",
      "(0.5,0.9999)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(eps_inf_nethept, [0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "display_for_tex_all(eps_inf_epin, [0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "display_for_tex_all(eps_inf_dblp, [0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "display_for_tex_all(eps_inf_orkut, [0.1, 0.2, 0.3, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_nethept = get_runtime_of_one_dataset(\"nethept\", rand_seeds_nethept, baseline_names=BASELINE_NAMES)\n",
    "runtime_epin = get_runtime_of_one_dataset(\"Epinions\", RAND_SEEDS, baseline_names=BASELINE_NAMES)\n",
    "runtime_dblp = get_runtime_of_one_dataset(\"DBLP\", RAND_SEEDS, baseline_names=BASELINE_NAMES)\n",
    "runtime_orkut = get_runtime_of_one_dataset(\"orkut\", RAND_SEEDS, baseline_names=BASELINE_NAMES)\n",
    "runtime_twitter = get_runtime_of_one_dataset(\"twitter\", RAND_SEEDS, baseline_names=BASELINE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAND:\n",
      "(5,9.9884)\n",
      "(10,11.7806)\n",
      "(20,16.2521)\n",
      "(30,19.319)\n",
      "(40,21.4907)\n",
      "(50,26.7651)\n",
      "OUTDEG:\n",
      "(5,29.7318)\n",
      "(10,29.9397)\n",
      "(20,31.1671)\n",
      "(30,29.7717)\n",
      "(40,30.6306)\n",
      "(50,32.1122)\n",
      "PROB:\n",
      "(5,52.178)\n",
      "(10,52.243)\n",
      "(20,53.6903)\n",
      "(30,52.2959)\n",
      "(40,53.2776)\n",
      "(50,52.6929)\n",
      "SINF:\n",
      "(5,22.5067)\n",
      "(10,52.9484)\n",
      "(20,146.2096)\n",
      "(30,271.7037)\n",
      "(40,447.0782)\n",
      "(50,641.6552)\n",
      "UINF:\n",
      "(5,24.3755)\n",
      "(10,57.2389)\n",
      "(20,153.8611)\n",
      "(30,285.171)\n",
      "(40,460.8796)\n",
      "(50,663.262)\n",
      "AIS-U:\n",
      "(5,29.7778)\n",
      "(10,68.4991)\n",
      "(20,177.9643)\n",
      "(30,318.9664)\n",
      "(40,504.696)\n",
      "(50,734.8698)\n",
      "IMA:\n",
      "(5,28.0647)\n",
      "(10,64.1785)\n",
      "(20,164.5828)\n",
      "(30,302.3398)\n",
      "(40,490.6986)\n",
      "(50,699.4426)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(runtime_twitter, [5, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,0.5917)\n",
      "(10,2.303)\n",
      "(20,9.3477)\n",
      "(30,21.0046)\n",
      "(40,38.5388)\n",
      "(50,58.8318)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex(runtime_nethept, \"IMA\", K_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SINF': array([ 0.7288724,  1.898564 ,  5.600618 , 11.020008 , 18.549964 ,\n",
       "        28.136332 ]),\n",
       " 'UINF': array([ 1.600365 ,  3.6491004,  9.106902 , 16.295694 , 25.62202  ,\n",
       "        36.99108  ]),\n",
       " 'AIS-U': array([ 1.801865 ,  4.0509464,  9.903734 , 17.479322 , 27.1619   ,\n",
       "        38.86684  ]),\n",
       " 'IMA': array([ 1.818009 ,  4.0850864,  9.97756  , 17.612822 , 27.38072  ,\n",
       "        39.31004  ])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_runtime_of_one_dataset(\"DBLP\", RAND_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nethept = get_inf_spread_of_one_dataset(\"nethept\", rand_seeds_nethept)\n",
    "res_epinions = get_inf_spread_of_one_dataset(\"Epinions\", RAND_SEEDS)\n",
    "res_dblp = get_inf_spread_of_one_dataset(\"DBLP\", RAND_SEEDS)\n",
    "# res_orkut = get_inf_spread_of_one_dataset(\"orkut\", RAND_SEEDS)\n",
    "# res_twitter = get_inf_spread_of_one_dataset(\"twitter\", RAND_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAND:\n",
      "(0,12050.22)\n",
      "(5,12053.06)\n",
      "(10,12061.2)\n",
      "(20,12082.0)\n",
      "(30,12081.26)\n",
      "(40,12081.02)\n",
      "(50,12086.58)\n",
      "OUTDEG:\n",
      "(0,12050.22)\n",
      "(5,12072.62)\n",
      "(10,12089.4)\n",
      "(20,12108.36)\n",
      "(30,12105.86)\n",
      "(40,12118.3)\n",
      "(50,12141.26)\n",
      "PROB:\n",
      "(0,12050.22)\n",
      "(5,12059.14)\n",
      "(10,12068.92)\n",
      "(20,12096.46)\n",
      "(30,12102.1)\n",
      "(40,12105.6)\n",
      "(50,12110.98)\n",
      "SINF:\n",
      "(0,12050.22)\n",
      "(5,12122.44)\n",
      "(10,12207.0)\n",
      "(20,12346.58)\n",
      "(30,12461.46)\n",
      "(40,12562.64)\n",
      "(50,12655.48)\n",
      "UINF:\n",
      "(0,12050.22)\n",
      "(5,12121.82)\n",
      "(10,12197.16)\n",
      "(20,12340.28)\n",
      "(30,12434.8)\n",
      "(40,12529.92)\n",
      "(50,12621.9)\n",
      "AIS-U:\n",
      "(0,12050.22)\n",
      "(5,12211.14)\n",
      "(10,12361.68)\n",
      "(20,12616.5)\n",
      "(30,12846.62)\n",
      "(40,13068.54)\n",
      "(50,13262.12)\n",
      "IMA:\n",
      "(0,12050.22)\n",
      "(5,12227.24)\n",
      "(10,12381.96)\n",
      "(20,12659.82)\n",
      "(30,12893.82)\n",
      "(40,13114.1)\n",
      "(50,13316.74)\n"
     ]
    }
   ],
   "source": [
    "display_for_tex_all(res_epinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
